#!/usr/bin/env -S uv run
# /// script
# requires-python = ">=3.12"
# dependencies = [
#     "python-telegram-bot[all]>=21.0",
#     "python-dotenv>=1.0.0",
#     "openai>=1.0.0",
# ]
# ///

"""
Simple Telegram Bot
"""

import asyncio
import json
import logging
import os
import shutil
import tempfile
from pathlib import Path
from dotenv import load_dotenv
from openai import AsyncOpenAI
from telegram import InlineKeyboardButton, InlineKeyboardMarkup, Update
from telegram.ext import (
    Application,
    CallbackQueryHandler,
    CommandHandler,
    ContextTypes,
    MessageHandler,
    filters,
)

# Load environment variables
load_dotenv()

# Enable logging
logging.basicConfig(
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    level=logging.INFO,
)
logger = logging.getLogger(__name__)


async def start(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """Send a message when the command /start is issued."""
    user = update.effective_user
    await update.message.reply_html(
        f"ÐŸÑ€Ð¸Ð²ÐµÑ‚ {user.mention_html()}!\n\n"
        f"Ð¯ Ð¿Ñ€Ð¾ÑÑ‚Ð¾Ð¹ Telegram Ð±Ð¾Ñ‚. ÐžÑ‚Ð¿Ñ€Ð°Ð²ÑŒ Ð¼Ð½Ðµ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ, Ð¸ Ñ Ð¾Ñ‚Ð²ÐµÑ‡Ñƒ!"
    )


async def help_command(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """Send a message when the command /help is issued."""
    max_size_mb = os.getenv("MAX_VIDEO_SIZE_MB", "100")
    help_text = f"""
Ð”Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹Ðµ ÐºÐ¾Ð¼Ð°Ð½Ð´Ñ‹:
/start - ÐÐ°Ñ‡Ð°Ñ‚ÑŒ Ñ€Ð°Ð±Ð¾Ñ‚Ñƒ Ñ Ð±Ð¾Ñ‚Ð¾Ð¼
/help - ÐŸÐ¾ÐºÐ°Ð·Ð°Ñ‚ÑŒ ÑÑ‚Ð¾ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ

Ð’Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ð¾ÑÑ‚Ð¸:
â€¢ ÐžÑ‚Ð¿Ñ€Ð°Ð²ÑŒÑ‚Ðµ Ð¼Ð½Ðµ Ð²Ð¸Ð´ÐµÐ¾, Ð¸ Ð²Ñ‹Ð±ÐµÑ€Ð¸Ñ‚Ðµ Ð²Ð°Ñ€Ð¸Ð°Ð½Ñ‚ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸:
  - ðŸŽµ Ð¢Ð¾Ð»ÑŒÐºÐ¾ Ð°ÑƒÐ´Ð¸Ð¾ - Ð¸Ð·Ð²Ð»ÐµÑ‡ÑŒ MP3
  - ðŸ“ Ð¢Ð¾Ð»ÑŒÐºÐ¾ Ñ‚Ñ€Ð°Ð½ÑÐºÑ€Ð¸Ð¿Ñ†Ð¸Ñ - Ñ‚ÐµÐºÑÑ‚ Ñ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ð¼Ð¸ Ð¼ÐµÑ‚ÐºÐ°Ð¼Ð¸
  - ðŸŽµðŸ“ ÐÑƒÐ´Ð¸Ð¾ + Ñ‚Ñ€Ð°Ð½ÑÐºÑ€Ð¸Ð¿Ñ†Ð¸Ñ - Ð¸ Ð°ÑƒÐ´Ð¸Ð¾, Ð¸ Ñ‚ÐµÐºÑÑ‚

Ð¤Ð¾Ñ€Ð¼Ð°Ñ‚ Ñ‚Ñ€Ð°Ð½ÑÐºÑ€Ð¸Ð¿Ñ†Ð¸Ð¸:
1. [00:00:00 - 00:00:15]
Ð¢ÐµÐºÑÑ‚ Ð¿ÐµÑ€Ð²Ð¾Ð¹ Ñ„Ñ€Ð°Ð·Ñ‹

2. [00:00:15 - 00:00:30]
Ð¢ÐµÐºÑÑ‚ Ð²Ñ‚Ð¾Ñ€Ð¾Ð¹ Ñ„Ñ€Ð°Ð·Ñ‹

â€¢ ÐœÐ°ÐºÑÐ¸Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ñ€Ð°Ð·Ð¼ÐµÑ€ Ð²Ð¸Ð´ÐµÐ¾: {max_size_mb} MB
â€¢ Ð”Ð»Ð¸Ð½Ð½Ñ‹Ðµ Ð°ÑƒÐ´Ð¸Ð¾ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸ Ñ€Ð°Ð·Ð±Ð¸Ð²Ð°ÑŽÑ‚ÑÑ Ð½Ð° Ñ‡Ð°ÑÑ‚Ð¸ Ð¿Ð¾ 5 Ð¼Ð¸Ð½ÑƒÑ‚
    """
    await update.message.reply_text(help_text)


async def echo(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """Echo the user message."""
    await update.message.reply_text(f"Ð’Ñ‹ Ð½Ð°Ð¿Ð¸ÑÐ°Ð»Ð¸: {update.message.text}")


async def get_audio_duration(audio_path: Path) -> float:
    """Get audio duration in seconds using ffprobe."""
    process = await asyncio.create_subprocess_exec(
        "ffprobe",
        "-v", "error",
        "-show_entries", "format=duration",
        "-of", "default=noprint_wrappers=1:nokey=1",
        str(audio_path),
        stdout=asyncio.subprocess.PIPE,
        stderr=asyncio.subprocess.PIPE,
    )
    stdout, stderr = await process.communicate()

    if process.returncode != 0:
        error_message = stderr.decode("utf-8", errors="ignore")
        raise RuntimeError(f"ffprobe failed: {error_message}")

    return float(stdout.decode().strip())


async def split_audio(
    audio_path: Path, chunk_duration: int = 300
) -> list[Path]:
    """Split audio into chunks of specified duration (seconds)."""
    # Get total duration first
    total_duration = await get_audio_duration(audio_path)

    chunks = []
    chunk_index = 0
    start_time = 0

    while start_time < total_duration:
        chunk_path = audio_path.parent / f"chunk_{chunk_index:03d}.mp3"

        # Use -ss (start time) and -t (duration) for precise splitting
        process = await asyncio.create_subprocess_exec(
            "ffmpeg",
            "-i", str(audio_path),
            "-ss", str(start_time),
            "-t", str(chunk_duration),
            "-acodec", "copy",  # Copy codec for speed
            "-y",  # Overwrite output file
            str(chunk_path),
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE,
        )

        await process.communicate()

        if process.returncode == 0 and chunk_path.exists():
            chunks.append(chunk_path)
            logger.debug(f"Created audio chunk: {chunk_path}")

        start_time += chunk_duration
        chunk_index += 1

    logger.info(f"Split audio into {len(chunks)} chunks")
    return chunks


async def transcribe_audio_chunk(
    client: AsyncOpenAI, chunk_path: Path, max_retries: int = 3
) -> dict:
    """Transcribe audio chunk with timestamps using OpenAI Whisper API."""
    for attempt in range(max_retries):
        try:
            with open(chunk_path, "rb") as audio_file:
                response = await client.audio.transcriptions.create(
                    model="whisper-1",
                    file=audio_file,
                    response_format="verbose_json",
                    timestamp_granularities=["segment"],
                )

            logger.debug(f"Successfully transcribed {chunk_path}")
            return response.model_dump()

        except Exception as e:
            error_type = type(e).__name__
            logger.warning(
                f"Transcription attempt {attempt + 1}/{max_retries} failed for "
                f"{chunk_path}: {error_type} - {str(e)}"
            )

            # Don't retry authentication errors
            if "authentication" in str(e).lower() or "api_key" in str(e).lower():
                raise

            # Retry with exponential backoff for other errors
            if attempt < max_retries - 1:
                wait_time = 2 ** attempt
                logger.info(f"Retrying in {wait_time} seconds...")
                await asyncio.sleep(wait_time)
            else:
                raise


def format_time(seconds: float) -> str:
    """Format seconds to HH:MM:SS."""
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    secs = int(seconds % 60)
    return f"{hours:02d}:{minutes:02d}:{secs:02d}"


def format_transcription_to_text(transcriptions: list[dict], chunk_offsets: list[float]) -> str:
    """Format transcription segments to readable text with timestamps."""
    formatted_lines = []
    segment_counter = 1

    for chunk_idx, transcription in enumerate(transcriptions):
        time_offset = chunk_offsets[chunk_idx]
        segments = transcription.get("segments", [])

        for segment in segments:
            start_time = segment.get("start", 0) + time_offset
            end_time = segment.get("end", 0) + time_offset
            text = segment.get("text", "").strip()

            if text:
                formatted_lines.append(
                    f"{segment_counter}. [{format_time(start_time)} - {format_time(end_time)}]\n{text}\n"
                )
                segment_counter += 1

    return "\n".join(formatted_lines)


async def transcribe_full_audio(
    audio_path: Path,
    status_message=None,
    chunk_duration: int = 300
) -> str:
    """Transcribe full audio file, splitting into chunks if necessary."""
    # Get OpenAI API key
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise ValueError("OPENAI_API_KEY not found in environment variables")

    client = AsyncOpenAI(api_key=api_key)

    # Check audio duration
    duration = await get_audio_duration(audio_path)
    logger.info(f"Audio duration: {duration:.1f} seconds")

    # Determine if splitting is needed
    if duration <= chunk_duration:
        # Transcribe directly without splitting
        logger.info("Audio is short, transcribing without splitting")
        if status_message:
            await status_message.edit_text("Ð¢Ñ€Ð°Ð½ÑÐºÑ€Ð¸Ð±Ð¸Ñ€ÑƒÑŽ Ð°ÑƒÐ´Ð¸Ð¾...")

        transcription = await transcribe_audio_chunk(client, audio_path)
        formatted_text = format_transcription_to_text([transcription], [0.0])
    else:
        # Split and transcribe each chunk
        num_chunks = int(duration // chunk_duration) + (1 if duration % chunk_duration > 0 else 0)
        logger.info(f"Splitting audio into {num_chunks} chunks")

        chunks = await split_audio(audio_path, chunk_duration)
        transcriptions = []
        chunk_offsets = []

        for idx, chunk_path in enumerate(chunks, 1):
            if status_message:
                await status_message.edit_text(
                    f"Ð¢Ñ€Ð°Ð½ÑÐºÑ€Ð¸Ð±Ð¸Ñ€ÑƒÑŽ Ð°ÑƒÐ´Ð¸Ð¾ (Ñ‡Ð°ÑÑ‚ÑŒ {idx}/{len(chunks)})..."
                )

            transcription = await transcribe_audio_chunk(client, chunk_path)
            transcriptions.append(transcription)
            # Calculate time offset for this chunk
            chunk_offsets.append((idx - 1) * chunk_duration)

            # Clean up chunk file
            try:
                chunk_path.unlink()
                logger.debug(f"Deleted chunk file: {chunk_path}")
            except Exception as e:
                logger.warning(f"Failed to delete chunk {chunk_path}: {e}")

        formatted_text = format_transcription_to_text(transcriptions, chunk_offsets)

    logger.info(f"Transcription completed. Total segments: {formatted_text.count('.')}")
    return formatted_text


async def send_transcription_as_file(
    query,
    transcription_text: str,
    temp_dir: Path
) -> None:
    """Send transcription as a text file."""
    transcription_path = temp_dir / "transcription.txt"

    # Write transcription to file
    transcription_path.write_text(transcription_text, encoding="utf-8")

    # Send as document
    with open(transcription_path, "rb") as f:
        await query.message.reply_document(
            document=f,
            filename="transcription.txt",
            caption=f"ðŸ“ Ð¢Ñ€Ð°Ð½ÑÐºÑ€Ð¸Ð¿Ñ†Ð¸Ñ ({len(transcription_text)} ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð²)"
        )

    logger.info(f"Sent transcription file ({len(transcription_text)} chars)")


def create_processing_options_keyboard() -> InlineKeyboardMarkup:
    """Create inline keyboard with processing options."""
    keyboard = [
        [InlineKeyboardButton("ðŸŽµ Ð¢Ð¾Ð»ÑŒÐºÐ¾ Ð°ÑƒÐ´Ð¸Ð¾", callback_data="audio_only")],
        [InlineKeyboardButton("ðŸ“ Ð¢Ð¾Ð»ÑŒÐºÐ¾ Ñ‚Ñ€Ð°Ð½ÑÐºÑ€Ð¸Ð¿Ñ†Ð¸Ñ", callback_data="transcription_only")],
        [InlineKeyboardButton("ðŸŽµðŸ“ ÐÑƒÐ´Ð¸Ð¾ + Ñ‚Ñ€Ð°Ð½ÑÐºÑ€Ð¸Ð¿Ñ†Ð¸Ñ", callback_data="audio_and_transcription")],
    ]
    return InlineKeyboardMarkup(keyboard)


async def handle_video(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """Handle video messages by showing processing options."""
    video = update.message.video
    user = update.effective_user

    # Get max video size from environment variable
    max_size_mb = int(os.getenv("MAX_VIDEO_SIZE_MB", "100"))
    max_size_bytes = max_size_mb * 1024 * 1024

    # Check file size
    if video.file_size > max_size_bytes:
        await update.message.reply_text(
            f"âŒ Ð’Ð¸Ð´ÐµÐ¾ ÑÐ»Ð¸ÑˆÐºÐ¾Ð¼ Ð±Ð¾Ð»ÑŒÑˆÐ¾Ðµ! ÐœÐ°ÐºÑÐ¸Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ñ€Ð°Ð·Ð¼ÐµÑ€: {max_size_mb} MB.\n"
            f"Ð Ð°Ð·Ð¼ÐµÑ€ Ð²Ð°ÑˆÐµÐ³Ð¾ Ð²Ð¸Ð´ÐµÐ¾: {video.file_size / 1024 / 1024:.1f} MB"
        )
        return

    # Check if ffmpeg is installed
    if not shutil.which("ffmpeg"):
        await update.message.reply_text(
            "âŒ ÐžÑˆÐ¸Ð±ÐºÐ°: FFmpeg Ð½Ðµ ÑƒÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½ Ð½Ð° ÑÐµÑ€Ð²ÐµÑ€Ðµ.\n"
            "ÐžÐ±Ñ€Ð°Ñ‚Ð¸Ñ‚ÐµÑÑŒ Ðº Ð°Ð´Ð¼Ð¸Ð½Ð¸ÑÑ‚Ñ€Ð°Ñ‚Ð¾Ñ€Ñƒ Ð´Ð»Ñ ÑƒÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ¸ FFmpeg."
        )
        logger.error("FFmpeg is not installed on the system")
        return

    logger.info(
        f"Received video from user {user.id} ({user.username}): "
        f"{video.file_size / 1024 / 1024:.1f} MB"
    )

    # Store video file_id in user_data for later processing
    context.user_data["video_file_id"] = video.file_id
    context.user_data["video_size"] = video.file_size

    # Show processing options
    keyboard = create_processing_options_keyboard()
    await update.message.reply_text(
        "Ð’Ñ‹Ð±ÐµÑ€Ð¸Ñ‚Ðµ Ð²Ð°Ñ€Ð¸Ð°Ð½Ñ‚ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸:",
        reply_markup=keyboard
    )


async def process_video_extraction(
    update: Update,
    context: ContextTypes.DEFAULT_TYPE,
    mode: str
) -> None:
    """Process video extraction based on selected mode.

    Modes:
    - audio_only: Extract and send MP3 only
    - transcription_only: Extract MP3, transcribe, send transcription, delete MP3
    - audio_and_transcription: Extract and send MP3, then transcribe and send transcription
    """
    query = update.callback_query
    user = query.from_user

    # Get video file_id from user_data
    video_file_id = context.user_data.get("video_file_id")
    video_size = context.user_data.get("video_size", 0)

    if not video_file_id:
        await query.edit_message_text("âŒ ÐžÑˆÐ¸Ð±ÐºÐ°: Ð²Ð¸Ð´ÐµÐ¾ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾. ÐŸÐ¾Ð¶Ð°Ð»ÑƒÐ¹ÑÑ‚Ð°, Ð¾Ñ‚Ð¿Ñ€Ð°Ð²ÑŒÑ‚Ðµ Ð²Ð¸Ð´ÐµÐ¾ Ð·Ð°Ð½Ð¾Ð²Ð¾.")
        return

    logger.info(f"Processing video for user {user.id} in mode: {mode}")

    # Send initial processing message
    status_message = await query.edit_message_text("ÐžÐ±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°ÑŽ Ð²Ð¸Ð´ÐµÐ¾.")

    # Create event to stop animation
    stop_animation = asyncio.Event()

    # Start animation task
    animation_task = asyncio.create_task(
        animate_processing_message(status_message, "ÐžÐ±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°ÑŽ Ð²Ð¸Ð´ÐµÐ¾", stop_animation)
    )

    temp_dir = None
    try:
        # Create temporary directory
        temp_dir = tempfile.mkdtemp(prefix="video_bot_")
        video_path = Path(temp_dir) / "input_video.mp4"
        audio_path = Path(temp_dir) / "output_audio.mp3"

        # Download video file
        logger.info(f"Downloading video to {video_path}")
        video_file = await context.bot.get_file(video_file_id)
        await video_file.download_to_drive(video_path)

        # Convert video to MP3 using ffmpeg
        logger.info(f"Converting video to MP3: {audio_path}")
        ffmpeg_process = await asyncio.create_subprocess_exec(
            "ffmpeg",
            "-i", str(video_path),
            "-vn",
            "-acodec", "libmp3lame",
            "-ar", "44100",
            "-ac", "2",
            "-ab", "192k",
            "-f", "mp3",
            str(audio_path),
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE,
        )

        stdout, stderr = await ffmpeg_process.communicate()

        if ffmpeg_process.returncode != 0:
            error_message = stderr.decode("utf-8", errors="ignore")
            logger.error(f"FFmpeg conversion failed: {error_message}")
            raise RuntimeError(f"FFmpeg failed with return code {ffmpeg_process.returncode}")

        # Stop animation
        stop_animation.set()
        await animation_task

        # Check if audio file was created
        if not audio_path.exists():
            raise FileNotFoundError("Audio file was not created")

        audio_size_mb = audio_path.stat().st_size / 1024 / 1024
        logger.info(f"Conversion successful. Audio size: {audio_size_mb:.1f} MB")

        # Process based on mode
        if mode == "audio_only":
            # Send audio only
            await status_message.edit_text("ÐžÑ‚Ð¿Ñ€Ð°Ð²Ð»ÑÑŽ Ð°ÑƒÐ´Ð¸Ð¾...")
            with open(audio_path, "rb") as audio_file:
                await query.message.reply_document(
                    document=audio_file,
                    filename="audio.mp3",
                    caption="ðŸŽµ ÐÑƒÐ´Ð¸Ð¾ Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¾ Ð¸Ð· Ð²Ð¸Ð´ÐµÐ¾",
                )
            await status_message.delete()
            logger.info(f"Audio sent successfully to user {user.id}")

        elif mode == "transcription_only":
            # Transcribe and send transcription only
            transcription = await transcribe_full_audio(
                audio_path,
                status_message=status_message
            )
            await status_message.edit_text("ÐžÑ‚Ð¿Ñ€Ð°Ð²Ð»ÑÑŽ Ñ‚Ñ€Ð°Ð½ÑÐºÑ€Ð¸Ð¿Ñ†Ð¸ÑŽ...")
            await send_transcription_as_file(query, transcription, Path(temp_dir))
            await status_message.delete()
            logger.info(f"Transcription sent successfully to user {user.id}")

        elif mode == "audio_and_transcription":
            # Send both audio and transcription
            await status_message.edit_text("ÐžÑ‚Ð¿Ñ€Ð°Ð²Ð»ÑÑŽ Ð°ÑƒÐ´Ð¸Ð¾...")
            with open(audio_path, "rb") as audio_file:
                await query.message.reply_document(
                    document=audio_file,
                    filename="audio.mp3",
                    caption="ðŸŽµ ÐÑƒÐ´Ð¸Ð¾ Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¾ Ð¸Ð· Ð²Ð¸Ð´ÐµÐ¾",
                )
            logger.info(f"Audio sent successfully to user {user.id}")

            # Now transcribe
            transcription = await transcribe_full_audio(
                audio_path,
                status_message=status_message
            )
            await status_message.edit_text("ÐžÑ‚Ð¿Ñ€Ð°Ð²Ð»ÑÑŽ Ñ‚Ñ€Ð°Ð½ÑÐºÑ€Ð¸Ð¿Ñ†Ð¸ÑŽ...")
            await send_transcription_as_file(query, transcription, Path(temp_dir))
            await status_message.delete()
            logger.info(f"Audio and transcription sent successfully to user {user.id}")

    except Exception as e:
        # Stop animation if it's still running
        stop_animation.set()
        await animation_task

        # Send error message to user
        error_msg = "âŒ ÐŸÑ€Ð¾Ð¸Ð·Ð¾ÑˆÐ»Ð° Ð¾ÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐµ Ð²Ð¸Ð´ÐµÐ¾."

        if isinstance(e, FileNotFoundError):
            error_msg += "\nÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ ÑÐ¾Ð·Ð´Ð°Ñ‚ÑŒ Ð°ÑƒÐ´Ð¸Ð¾ Ñ„Ð°Ð¹Ð»."
        elif isinstance(e, RuntimeError):
            error_msg += "\nÐžÑˆÐ¸Ð±ÐºÐ° ÐºÐ¾Ð½Ð²ÐµÑ€Ñ‚Ð°Ñ†Ð¸Ð¸ Ð²Ð¸Ð´ÐµÐ¾."
        elif "OPENAI_API_KEY" in str(e):
            error_msg += "\nÐžÑˆÐ¸Ð±ÐºÐ° ÐºÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ð¸ OpenAI API."
        elif "authentication" in str(e).lower():
            error_msg += "\nÐžÑˆÐ¸Ð±ÐºÐ° Ð°ÑƒÑ‚ÐµÐ½Ñ‚Ð¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ð¸ OpenAI API."
        else:
            error_msg += f"\n{type(e).__name__}"

        await status_message.edit_text(error_msg)
        logger.error(f"Error processing video: {e}", exc_info=True)

    finally:
        # Clean up temporary files
        if temp_dir and Path(temp_dir).exists():
            try:
                shutil.rmtree(temp_dir)
                logger.debug(f"Cleaned up temporary directory: {temp_dir}")
            except Exception as e:
                logger.error(f"Failed to clean up temp directory: {e}")


async def handle_callback_query(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """Handle callback queries from inline keyboard."""
    query = update.callback_query
    await query.answer()

    # Extract callback data
    callback_data = query.data

    if callback_data in ["audio_only", "transcription_only", "audio_and_transcription"]:
        await process_video_extraction(update, context, callback_data)
    else:
        await query.edit_message_text("âŒ ÐÐµÐ¸Ð·Ð²ÐµÑÑ‚Ð½Ð°Ñ ÐºÐ¾Ð¼Ð°Ð½Ð´Ð°.")


async def animate_processing_message(
    message, base_text: str, stop_event: asyncio.Event
) -> None:
    """Animate processing message by cycling through dots."""
    dots = [".", "..", "..."]
    idx = 0

    while not stop_event.is_set():
        try:
            await message.edit_text(f"{base_text}{dots[idx]}")
            idx = (idx + 1) % len(dots)
            await asyncio.sleep(1)
        except Exception as e:
            # Ignore errors during animation (e.g., message not modified)
            logger.debug(f"Animation error: {e}")
            await asyncio.sleep(1)


async def error_handler(update: object, context: ContextTypes.DEFAULT_TYPE) -> None:
    """Log errors caused by updates."""
    logger.error("Exception while handling an update:", exc_info=context.error)


def main() -> None:
    """Start the bot."""
    # Get bot token from environment variable
    token = os.getenv("TELEGRAM_BOT_TOKEN")

    if not token:
        logger.error("TELEGRAM_BOT_TOKEN not found in environment variables!")
        return

    # Create the Application
    application = Application.builder().token(token).build()

    # Register command handlers
    application.add_handler(CommandHandler("start", start))
    application.add_handler(CommandHandler("help", help_command))

    # Register callback query handler for inline buttons
    application.add_handler(CallbackQueryHandler(handle_callback_query))

    # Register video handler
    application.add_handler(MessageHandler(filters.VIDEO, handle_video))

    # Register message handler for text messages
    application.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, echo))

    # Register error handler
    application.add_error_handler(error_handler)

    # Start the bot
    logger.info("Starting bot...")
    application.run_polling(allowed_updates=Update.ALL_TYPES)


if __name__ == "__main__":
    main()
